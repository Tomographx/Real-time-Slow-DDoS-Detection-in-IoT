{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31521366",
   "metadata": {},
   "source": [
    "# Data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f72a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV files: 169\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "csv_files = []\n",
    "for root, dirs, files in os.walk('F:\\dataset\\csv\\CICIoT2023'):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_files.append(os.path.join(root, file))\n",
    "print(f\"Total CSV files: {len(csv_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "223ed746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines (including headers): 46686748\n"
     ]
    }
   ],
   "source": [
    "total_lines = 0\n",
    "for file_path in csv_files:\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        total_lines += sum(1 for _ in f)\n",
    "print(f\"Total lines (including headers): {total_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cb1d0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged file saved to: F:\\dataset\\merged csv\\merged.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = r'F:\\dataset\\merged csv'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "merged_file_path = os.path.join(output_dir, 'merged.csv')\n",
    "\n",
    "# Corrected: robust header skipping logic\n",
    "header_written = False\n",
    "\n",
    "with open(merged_file_path, 'w', encoding='utf-8') as fout:\n",
    "    for file_path in sorted(csv_files):\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as fin:\n",
    "            lines = fin.readlines()\n",
    "            if not header_written:\n",
    "                fout.write(lines[0])  # write header once\n",
    "                header_written = True\n",
    "            fout.writelines(lines[1:])  # skip first line (header) in each file\n",
    "\n",
    "print(f\"âœ… Merged file saved to: {merged_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5d62e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Merged CSV has 46686579 rows and 47 columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'F:\\dataset\\merged csv\\merged.csv')\n",
    "\n",
    "print(f\"ðŸ“ Merged CSV has {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc1de0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first few rows:\n",
      "   flow_duration  Header_Length  Protocol Type  Duration         Rate  \\\n",
      "0       0.000000          54.00           6.00     64.00     0.329807   \n",
      "1       0.000000          57.04           6.33     64.00     4.290556   \n",
      "2       0.000000           0.00           1.00     64.00    33.396799   \n",
      "3       0.328175       76175.00          17.00     64.00  4642.133010   \n",
      "4       0.117320         101.73           6.11     65.91     6.202211   \n",
      "\n",
      "         Srate  Drate  fin_flag_number  syn_flag_number  rst_flag_number  ...  \\\n",
      "0     0.329807    0.0              1.0              0.0              1.0  ...   \n",
      "1     4.290556    0.0              0.0              0.0              0.0  ...   \n",
      "2    33.396799    0.0              0.0              0.0              0.0  ...   \n",
      "3  4642.133010    0.0              0.0              0.0              0.0  ...   \n",
      "4     6.202211    0.0              0.0              1.0              0.0  ...   \n",
      "\n",
      "         Std  Tot size           IAT  Number   Magnitue     Radius  \\\n",
      "0   0.000000     54.00  8.334383e+07     9.5  10.392305   0.000000   \n",
      "1   2.822973     57.04  8.292607e+07     9.5  10.464666   4.010353   \n",
      "2   0.000000     42.00  8.312799e+07     9.5   9.165151   0.000000   \n",
      "3   0.000000     50.00  8.301570e+07     9.5  10.000000   0.000000   \n",
      "4  23.113111     57.88  8.297300e+07     9.5  11.346876  32.716243   \n",
      "\n",
      "    Covariance  Variance  Weight             label  \n",
      "0     0.000000      0.00  141.55  DDoS-RSTFINFlood  \n",
      "1   160.987842      0.05  141.55     DoS-TCP_Flood  \n",
      "2     0.000000      0.00  141.55   DDoS-ICMP_Flood  \n",
      "3     0.000000      0.00  141.55     DoS-UDP_Flood  \n",
      "4  3016.808286      0.19  141.55     DoS-SYN_Flood  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Preview of the first few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "602f2f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label column name: label\n",
      "Found 34 unique labels (attack types):\n",
      "\n",
      "DDoS-ICMP_Flood                --> 7,200,504 samples\n",
      "DDoS-UDP_Flood                 --> 5,412,287 samples\n",
      "DDoS-TCP_Flood                 --> 4,497,667 samples\n",
      "DDoS-PSHACK_Flood              --> 4,094,755 samples\n",
      "DDoS-SYN_Flood                 --> 4,059,190 samples\n",
      "DDoS-RSTFINFlood               --> 4,045,285 samples\n",
      "DDoS-SynonymousIP_Flood        --> 3,598,138 samples\n",
      "DoS-UDP_Flood                  --> 3,318,595 samples\n",
      "DoS-TCP_Flood                  --> 2,671,445 samples\n",
      "DoS-SYN_Flood                  --> 2,028,834 samples\n",
      "BenignTraffic                  --> 1,098,195 samples\n",
      "Mirai-greeth_flood             --> 991,866 samples\n",
      "Mirai-udpplain                 --> 890,576 samples\n",
      "Mirai-greip_flood              --> 751,682 samples\n",
      "DDoS-ICMP_Fragmentation        --> 452,489 samples\n",
      "MITM-ArpSpoofing               --> 307,593 samples\n",
      "DDoS-UDP_Fragmentation         --> 286,925 samples\n",
      "DDoS-ACK_Fragmentation         --> 285,104 samples\n",
      "DNS_Spoofing                   --> 178,911 samples\n",
      "Recon-HostDiscovery            --> 134,378 samples\n",
      "Recon-OSScan                   --> 98,259 samples\n",
      "Recon-PortScan                 --> 82,284 samples\n",
      "DoS-HTTP_Flood                 --> 71,864 samples\n",
      "VulnerabilityScan              --> 37,382 samples\n",
      "DDoS-HTTP_Flood                --> 28,790 samples\n",
      "DDoS-SlowLoris                 --> 23,426 samples\n",
      "DictionaryBruteForce           --> 13,064 samples\n",
      "BrowserHijacking               --> 5,859 samples\n",
      "CommandInjection               --> 5,409 samples\n",
      "SqlInjection                   --> 5,245 samples\n",
      "XSS                            --> 3,846 samples\n",
      "Backdoor_Malware               --> 3,218 samples\n",
      "Recon-PingSweep                --> 2,262 samples\n",
      "Uploading_Attack               --> 1,252 samples\n"
     ]
    }
   ],
   "source": [
    "label_column = df.columns[46]\n",
    "print(f\"Label column name: {label_column}\")\n",
    "\n",
    "# Get counts of each unique label\n",
    "label_counts = df[label_column].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(f\"Found {len(label_counts)} unique labels (attack types):\\n\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label:<30} --> {count:,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "68253b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 1121621 rows and 47 columns.\n",
      "label\n",
      "BenignTraffic     1098195\n",
      "DDoS-SlowLoris      23426\n",
      "Name: count, dtype: int64\n",
      "Filtered CSV saved to: F:\\dataset\\merged csv\\cleaned_datase.csv\n"
     ]
    }
   ],
   "source": [
    "# Get label column (47th column, index 46)\n",
    "label_column = df.columns[46]\n",
    "\n",
    "# Filter only the required labels\n",
    "filtered_df = df[df[label_column].isin(['DDoS-SlowLoris', 'BenignTraffic'])]\n",
    "\n",
    "# Show result\n",
    "print(f\"Filtered dataset contains {filtered_df.shape[0]} rows and {filtered_df.shape[1]} columns.\")\n",
    "print(filtered_df[label_column].value_counts())\n",
    "\n",
    "# Optional: Save filtered data\n",
    "filtered_df.to_csv(r'F:\\dataset\\merged csv\\cleaned_dataset.csv', index=False)\n",
    "print(\"Filtered CSV saved to: F:\\\\dataset\\\\merged csv\\\\cleaned_datase.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca2957e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced dataset label counts:\n",
      "label\n",
      "DDoS-SlowLoris    23426\n",
      "BenignTraffic     23426\n",
      "Name: count, dtype: int64\n",
      "Balanced CSV saved to: F:\\dataset\\merged csv\\dataset_balance.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "slowloris_df = filtered_df[filtered_df[label_column] == 'DDoS-SlowLoris']\n",
    "benign_df = filtered_df[filtered_df[label_column] == 'BenignTraffic']\n",
    "\n",
    "\n",
    "benign_sampled_df = benign_df.sample(n=slowloris_df.shape[0], random_state=42)\n",
    "\n",
    "\n",
    "balanced_df = pd.concat([slowloris_df, benign_sampled_df])\n",
    "\n",
    "\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"\\nBalanced dataset label counts:\")\n",
    "print(balanced_df[label_column].value_counts())\n",
    "\n",
    "\n",
    "balanced_df.to_csv(r'F:\\dataset\\merged csv\\dataset_balance.csv', index=False)\n",
    "print(\"Balanced CSV saved to: F:\\\\dataset\\\\merged csv\\\\dataset_balance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
